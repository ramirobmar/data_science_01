{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4fc38b3b-f51a-4fde-add8-ccda28357144",
    "_uuid": "00b8832b3c051b1b4ea6e8c568c57c96fd66494f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is a part of the solution, for test the application we are developed two training dataset before and\n",
    "#we are use a dataset that contain an image with anamalies to check the correct functionality of\n",
    "#functions an libraries developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "5dcc3732-bef3-44a9-a9f7-c0031bdf493c",
    "_uuid": "49dcf09247abfef871c28bb767936b60baa3036f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import array, asarray, ma, zeros, sum\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.cluster.vq import vq, kmeans, whiten, kmeans2\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1ddb84fa-49c6-4b2e-ab83-5b9fa9be0ac8",
    "_uuid": "4979653104b147671f429590b937b1c5309abd9a",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datainput\n",
      "highresolution\n",
      "modeldbscan\n",
      "modelstraining\n",
      "passenger-screening-algorithm-challenge\n",
      "\n",
      "#################################\n",
      "Directory application structure\n",
      "#################################\n",
      "0a83698bce92a6824dcc37c1d7fc31f5.csv\n",
      "\n",
      "model_kmeans.csv\n",
      "\n",
      "0a83698bce92a6824dcc37c1d7fc31f5_high_resolution.csv\n",
      "\n",
      "model_dbscan.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "os.system('mkdir ./output')\n",
    "os.system('mkdir ./output/Threats')\n",
    "os.system('mkdir ./output/NoThreats')\n",
    "os.system('mkdir ./training')\n",
    "os.system('mkdir ./results')\n",
    "os.system('mkdir ./models')\n",
    "os.system('mkdir ./input')\n",
    "print(\"#################################\")\n",
    "print(\"Directory application structure\")\n",
    "print(\"#################################\")\n",
    "print(check_output([\"ls\", \"../input/datainput\"]).decode(\"utf8\"))\n",
    "print(check_output([\"ls\", \"../input/modelstraining\"]).decode(\"utf8\"))\n",
    "print(check_output([\"ls\", \"../input/highresolution\"]).decode(\"utf8\"))\n",
    "print(check_output([\"ls\", \"../input/modeldbscan\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "cc483fe8-8fe8-46ab-a997-133808119ba2",
    "_uuid": "9b4ace7f17f9d1471b08539b68bc5a940cdefba1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Name: cluster_analisys\n",
    "# Autor: Ramiro Bueno Martínez\n",
    "# Date: 27/11/2017\n",
    "##########################################################################################################\n",
    "def cluster_analisys(subset):\n",
    "\ttry:\n",
    "\t\twhitened = whiten(subset)\n",
    "\t\tcodebook,distortion = kmeans(whitened,5)\n",
    "\t\treturn distortion,whitened,codebook\n",
    "\texcept Exception as exception:\n",
    "\t\tprint (\"cluster_analisys: Excepcion {0}\".format(exception))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "54930a93-06c9-466c-831e-b6d2e0951261",
    "_uuid": "f3c1d4092c804a3b9df630e893a88933662779ec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_PHOTO_FILES = '.'\n",
    "NAME_FILE_MODEL_KMEANS = '../input/modelstraining/model_kmeans.csv'\n",
    "NAME_FILE_MODEL_DBSCAN = '../input/modeldbscan/model_dbscan.csv'\n",
    "DATA_FOR_PREDICTION_FILE = '../input/datainput/0a83698bce92a6824dcc37c1d7fc31f5.csv'\n",
    "DATA_FOR_PREDICTION_FILE_FILTERED = '../input/highresolution/0a83698bce92a6824dcc37c1d7fc31f5_high_resolution.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ff3d8bf0-34f0-4900-a7ad-4e2f262735da",
    "_uuid": "cc11b623c77913c54873338c39b5608d678ac7d2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Name: creation_dataset_threats\n",
    "# Autor: Ramiro Bueno Martínez\n",
    "# Date: 27/11/2017\n",
    "# Description: The goal of this function is the finding of possible anomalies in the different kind of\n",
    "# images generated with the body scannes, using clustering techniques based in the analysis of different\n",
    "# part of the body and different possition imagees\n",
    "############################################################################################################\n",
    "def creation_dataset_threats(dataset, FILENAME, highcontrast=False):\n",
    "\n",
    "\t\t# We have an image with a matrix structure of 660 rows by 512 colums\n",
    "\t\t# This function send this matrix to a algorithm to obtain different centroids and the distorsion\n",
    "\t\t# values that we will use to find possible anomalies \n",
    "\t\t#Initialization variables\n",
    "\t\tm_distortion = 0\n",
    "\t\tcol = 0\n",
    "\t\trow = 0\n",
    "\t\tnth = 0\n",
    "\t\t\n",
    "\t\t\n",
    "\t\toutput_dir = PATH_PHOTO_FILES + '/' + 'output'\n",
    "\t\tinput_dir = PATH_PHOTO_FILES + '/' + 'input'\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tname_of_photo_file = input_dir + '/' + FILENAME\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tprint (\"Starting clustering Analisys: {0}\".format(FILENAME))\n",
    "\t\t\n",
    "\t\t#Only they are going to analyse 4 Image Front-Behind-Left-Right\n",
    "\t\trange_images = [0,4,8,12]\n",
    "\t\ttry:\n",
    "\t\t\t\n",
    "\t\t\t#for nth in range(16):\n",
    "\t\t\tfor nth in range_images:\n",
    "\t\t\t\n",
    "\t\t\t\tan_img = get_single_image(name_of_photo_file, nth)  \t\t\t\t#returns the nth=3 image from the image stack\n",
    "\t\t\t\t\n",
    "\t\t\t\t#Prueba mejorando la resolución de la imagen \n",
    "\t\t\t\tif highcontrast == True:\n",
    "\t\t\t\t\timg_rescaled = convert_to_grayscale(an_img)\n",
    "\t\t\t\t\tan_img = spread_spectrum(img_rescaled)\n",
    "\t\t\n",
    "\t\t\t\tdata_array = np.array(an_img)\n",
    "\t\t\t\tdistortion_array[nth,1],whitened,codebook= cluster_analisys(data_array)\n",
    "\t\t\t\tm_distortion = m_distortion + distortion_array[nth,1]\n",
    "\t\t\t\trecord = pd.DataFrame([[FILENAME, nth, distortion_array[nth,1],'nothreat']],columns=['Name','Nth','Distortion','Threat'])\n",
    "\t\t\t\tdataset = dataset.append(record,ignore_index=True)\n",
    "\t\t\t\tcol = col + 1\n",
    "\t\t\t\tif col % 4 == 0:\n",
    "\t\t\t\t\trow = row + 1\n",
    "\t\t\t\t\tcol = 0\n",
    "\t\t\t\tprint(\".\")\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\t#plt.show()\n",
    "\t\t\t#print(distortion_array)\n",
    "\t\t\t####################################################\n",
    "\t\t\t#Esta parte se puede optimizar con las funciones DataFrame.mean()\n",
    "\t\t\tmedia = 0\n",
    "\t\t\tmedia = m_distortion / 4\n",
    "\t\t\t#print (\"Mean Value of Distortion: {0} \\n\".format(media))\n",
    "\t\t\t############################################################################################\n",
    "\t\t\t# At this point we are finding possible anomalies of different distortion measurement with\n",
    "\t\t\t# respect to a threshold fixed before\n",
    "\t\t\t############################################################################################\n",
    "\t\t\tpercent = 17  # 17% of the mean value we suppose that its a wrong value \n",
    "\t\t\t############################################################################################\n",
    "\t\t\tthreshold_thread = media * percent / 100\n",
    "\t\t\tfor i in range(len(dataset)):\n",
    "\t\t\t\tif abs(media - dataset.loc[i,('Distortion')]) > threshold_thread: \n",
    "\t\t\t\t\tdataset.loc[i,('Threat')] = 'threat'\n",
    "\t\t\t\tif abs(media - dataset.loc[i,('Distortion')]) <= threshold_thread:\n",
    "\t\t\t\t\tdataset.loc[i,('Threat')] = 'nothreat'\n",
    "\t\t\t\t\t\n",
    "\t\texcept Exception as exception:\n",
    "\t\t\tprint (\"Function [creation_dataset_threats]: Excepcion {0}\".format(exception))\n",
    "\t\t\tsys.exit(1)\n",
    "\t\tfinally:\n",
    "\t\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "4d228864-dabb-43dd-9c1e-2297e5d15614",
    "_uuid": "591e1a1258f7a1700a99059875ffd1f5f721fa3f",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ANALISIS_TOOL: Initialization of the system...\n",
      "Starting model creation based in Support Vector Machine Algorithm ......\n",
      "Created model ..........\n",
      "Starting the process of detection threats .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/cluster/vq.py:141: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################################\n",
      "Results SECTION 1: Prediction with svm algorithm\n",
      "#############################################################\n",
      "Passenger datainput it could suppose a thread at Body Scanner 3\n",
      "###################################################################################\n",
      "Results SECTION 2: Prediction with svm algorithm over image with high contrast\n",
      "####################################################################################\n",
      "Passenger highresolution it could suppose a thread at Body Scanner 3\n",
      "#########################################################################\n",
      "Improvement with the filtering image -55.25229867048024 %\n",
      "#########################################################################\n",
      "DATA_ANALISIS_TOOL: That's all, the programe is ended\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tprint (\"DATA_ANALISIS_TOOL: Initialization of the system...\")\n",
    "\ttry:\n",
    "\t\t##############################################################################################\n",
    "\t\t#APPLICATION OF DIFFERENT KIND OF MODELS TO DETECT POSSIBLE THREATS\n",
    "\t\t##############################################################################################\n",
    "\t\tprint(\"Starting model creation based in Support Vector Machine Algorithm ......\")\n",
    "\t\tdataset = pd.read_csv(NAME_FILE_MODEL_KMEANS, sep=',')\n",
    "\t\tn_samp = len(dataset)\n",
    "\t\tX, y = make_classification(n_samples=n_samp, n_features=4, random_state=0)\n",
    "\t\trow = 0\n",
    "\t\tfor index in range(len(dataset)):\n",
    "\t\t\tX[row][0]=dataset.loc[index,'Nth']\n",
    "\t\t\tX[row][1]=dataset.loc[index,'Distortion']\n",
    "\t\t\tX[row][2]=dataset.loc[index,'Difference']\n",
    "\t\t\tX[row][3]=dataset.loc[index,'Mean']\n",
    "\t\t\tif dataset.loc[index,'Threat'] == 'threat':\n",
    "\t\t\t\ty[index]=np.int32(1)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty[index]=np.int32(0)\n",
    "\t\t\t\n",
    "\t\t\trow = row + 1\n",
    "\t\tclf = LinearSVC(random_state=0)\n",
    "\t\tclf.fit(X, y)\n",
    "\t\tprint(\"Created model ..........\")\n",
    "\t\tprint(\"Starting the process of detection threats .....\")\n",
    "\t\tdataset_new = pd.read_csv(DATA_FOR_PREDICTION_FILE, sep=',')\n",
    "\t\tdata_array = np.array(dataset_new)\n",
    "\t\tdistortion,whitened,codebook = cluster_analisys(data_array)\n",
    "\t\tmean_value = dataset['Distortion'].mean()\n",
    "\t\tdifference = abs(mean_value - distortion)\n",
    "\t\tdistortion_indicator1 = difference / mean_value * 100\n",
    "\t\tnth = 3\n",
    "\t\tresult_of_prediction = clf.predict([[nth,distortion,difference,mean_value]]) \t\n",
    "\t\tname_file = DATA_FOR_PREDICTION_FILE\n",
    "\t\tname_passenger = str(name_file).split('/')\n",
    "\t\tname_passenger = str(name_passenger[2]).split('.')\n",
    "\t\tname_passenger = name_passenger[0]\n",
    "\t\tprint(\"#############################################################\")\n",
    "\t\tprint(\"Results SECTION 1: Prediction with svm algorithm\")\n",
    "\t\tprint(\"#############################################################\")\t\n",
    "\t\tif result_of_prediction == True:\n",
    "\t\t\tprint(\"Passenger {0} it could suppose a thread at Body Scanner {1}\".format(name_passenger,3))\n",
    "\t\t\tthreat_result = True\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Passenger {0} it could suppose a thread at Body Scanner {1}\".format(name_passenger,3))\n",
    "\t\tname_file = DATA_FOR_PREDICTION_FILE_FILTERED\n",
    "\t\tdataset_new = pd.read_csv(DATA_FOR_PREDICTION_FILE_FILTERED, sep=',')\n",
    "\t\tdata_array = np.array(dataset_new)\n",
    "\t\tdistortion,whitened,codebook = cluster_analisys(data_array)\n",
    "\t\tmean_value = dataset['Distortion'].mean()\n",
    "\t\tdifference = abs(mean_value - distortion)\n",
    "\t\tdistortion_indicator2 = difference / mean_value * 100\n",
    "\t\tnth = 3\n",
    "\t\tresult_of_prediction = clf.predict([[nth,distortion,difference,mean_value]]) \t\n",
    "\t\tname_file = DATA_FOR_PREDICTION_FILE_FILTERED\n",
    "\t\tname_passenger = str(name_file).split('/')\n",
    "\t\tname_passenger = str(name_passenger[2]).split('.')\n",
    "\t\tname_passenger = name_passenger[0]\n",
    "\t\t\n",
    "\t\tprint(\"###################################################################################\")\n",
    "\t\tprint(\"Results SECTION 2: Prediction with svm algorithm over image with high contrast\")\n",
    "\t\tprint(\"####################################################################################\")\t\n",
    "\t\tif result_of_prediction == True:\n",
    "\t\t\tprint(\"Passenger {0} it could suppose a thread at Body Scanner {1}\".format(name_passenger,3))\n",
    "\t\t\tthreat_result = True\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Passenger {0} it could suppose a thread at Body Scanner {1}\".format(name_passenger,3))\n",
    "\t\tprint(\"#########################################################################\")\n",
    "\t\timprovement = distortion_indicator2 - distortion_indicator1\n",
    "\t\tprint(\"Improvement with the filtering image {0} %\".format(improvement))\n",
    "\t\tprint(\"#########################################################################\")\n",
    " \t\t#print(\"Results: {0} % of image distortion without\".format(distortion_indicator1))\n",
    " \t\t#print(\"Results: {0} % of image distortion with\".format(distortion_indicator2))\n",
    " \t\t#print(\"Improvement {0}\".foramt(distortion_indicator2-distortion_indicator1))\n",
    "\t\tsys.exit(0)  #At the demo only the programme do a unique prediction\n",
    "\texcept Exception as exception:\n",
    "\t\tprint (\"DATA_ANALISIS_TOOL: Excepcion {0}\".format(exception))\n",
    "\t\tsys.exit(1)\n",
    "\tfinally:\n",
    "\t\tprint(\"DATA_ANALISIS_TOOL: That's all, the programe is ended\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "6681d663-5eae-47be-b71a-03378efd5119",
    "_uuid": "656d24466b0910ee405f36d37e96a60f6e6b2557",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SECTION: 3 APPLICATION DB-SCAN TECHNIQUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "155879ba-e5e3-4485-bf0e-5c4f351b2347",
    "_uuid": "e9a1ea50630f15fb58d04671cf0f7166e5702245",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dbscan_analysis(data_array):\n",
    "\t\tprint(\"#############################################################\")\n",
    "\t\tprint(\"Results: SECTION 3 Techniques dbscan application\")\n",
    "\t\tprint(\"#############################################################\")\t\n",
    "\t\twhitened = whiten(data_array)\n",
    "\t\trecord = pd.DataFrame({'x_axes': whitened[:, 0],'y_axes':whitened[:, 1]})\n",
    "\t\tcenters = np.array(record)\n",
    "\t\tX, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)\n",
    "\t\tX = StandardScaler().fit_transform(X)\n",
    "\t\tdb = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "\t\tcore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "\t\tcore_samples_mask[db.core_sample_indices_] = True\n",
    "\t\tlabels = db.labels_\n",
    "\t\tn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\t\tName = ''\n",
    "\t\tnClusters = n_clusters_\n",
    "\t\tHomegeneity = metrics.homogeneity_score(labels_true, labels)\n",
    "\t\tCompleteness = \tmetrics.completeness_score(labels_true, labels)\n",
    "\t\tVmeasure = metrics.v_measure_score(labels_true, labels)\n",
    "\t\tARIndex = metrics.adjusted_rand_score(labels_true, labels)\n",
    "\t\tAMInformation = \tmetrics.adjusted_mutual_info_score(labels_true, labels)\n",
    "\t\tSilCoefficient = metrics.silhouette_score(X, labels)\n",
    "\t\tThreat = 'threat'\n",
    "\t\tdataset = pd.read_csv(NAME_FILE_MODEL_DBSCAN, sep=',')\n",
    "\t\tn_samp = len(dataset)\n",
    "\t\tX, y = make_classification(n_samples=n_samp, n_features=7, random_state=0)\n",
    "\t\trow = 0\n",
    "\t\tprint('Starting the process of creation of record testing {0}'.format(len(dataset)))\n",
    "\t\tfor index in range(len(dataset)):\n",
    "\t\t\tX[row][0]=dataset.loc[index,'nClusters']\n",
    "\t\t\tX[row][1]=dataset.loc[index,'Homegeneity']\n",
    "\t\t\tX[row][2]=dataset.loc[index,'Completeness']\n",
    "\t\t\tX[row][3]=dataset.loc[index,'Vmeasure']\n",
    "\t\t\tX[row][4]=dataset.loc[index,'ARIndex']\n",
    "\t\t\tX[row][5]=dataset.loc[index,'AMInformation']\n",
    "\t\t\tX[row][6]=dataset.loc[index,'SilCoefficient']\n",
    "\t\t\tif dataset.loc[index,'Threat'] == 'threat':\n",
    "\t\t\t\tprint(\"Finded a case of threat {0}\".format(index))\n",
    "\t\t\t\ty[index]=np.int32(1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Finded a case of No threat\")\n",
    "\t\t\t\ty[index]=np.int32(0)\n",
    "\t\t\trow = row + 1\n",
    "\t\tprint(\"Starting the process of model scan creation\")\n",
    "\t\tclf = LinearSVC(random_state=0)\n",
    "\t\tclf.fit(X, y)\n",
    "\t\tprint(\"Created model dbscan..........\")\n",
    "\t\t\n",
    "\t\tprint('Results: {0}'.format(Homegeneity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
